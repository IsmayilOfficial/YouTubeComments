{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scraping and Analyzing Youtube Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will give a detailed walktrough of how to use the Tuber package (https://cran.r-project.org/web/packages/tuber/tuber.pdf) to extract youtube comments, format them and give some basic examples for analysis of text and contained emojis.\n",
    "\n",
    "The script is a preliminary part of an ongoing research project (https://www.researchgate.net/project/Methods-and-Tools-for-Automatic-Sampling-and-Analysis-of-YouTube-Comments) and will be subject to change. If you use substantive parts of this script as part of your own research, please cite it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to set up our R environment correctly so we are able to use the tuber package and all other\n",
    "necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all objects from the current global environment\n",
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a working directory in a GUI window\n",
    "dir <- choose.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing working directory\n",
    "setwd(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove directory string from global environment\n",
    "rm(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options parameter so that textstrings are not interpreted as factor variables\n",
    "options(stringsAsFactors = FALSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of necessary packages for data extraction and analysis\n",
    "packages <- c(\"devtools\",\n",
    "              \"tm\",\n",
    "              \"quanteda\",\n",
    "              \"tuber\",\n",
    "              \"qdapRegex\",\n",
    "              \"rlang\",\n",
    "              \"purrr\",\n",
    "              \"ggplot2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all packages in list\n",
    "install.packages(packages,repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attaching all packages in list\n",
    "lapply(packages, library, character.only = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the list-variable with the package names from the global environment\n",
    "rm(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the emo package from github (not on CRAN yet)\n",
    "devtools::install_github(\"hadley/emo\")\n",
    "library(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if packages have been properly attached\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentication for Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to data from Youtube, we have to use the Youtube API (https://developers.google.com/youtube/v3/).\n",
    "To do this, we need a token that identifies us when using the API so Youtube can be sure\n",
    "that their Terms and Conditions are respected (e.g. there is a certain limit how much data you can access in\n",
    "a given timeframe). We´ll go about this step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you do not have a google account that you are willing to use for this project, you have to create a new one here https://accounts.google.com/signup/v2/webcreateaccount?hl=en-GB&flowName=GlifWebSignIn&flowEntry=SignUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With your google account, you have to create a \"Google Project\". Detailed instructions can be found here: https://www.youtube.com/watch?v=Im69kzhpR3I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the credentials of the project-account to authenticate your R-session. By doing this, you allow R access to all data on youtube,that you would be able to see if you went to the site logged in with this account. Be carefull to **NOT** share there credentials with anyoneelse you don´t want to be able to log into this account. To get the credentials, on the project website:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- activate Youtube Date API v.3\n",
    "- click on create credentials\n",
    "- select authentification trough webbrowser\n",
    "- select access private data with users permission\n",
    "- set \"autorised referral URL\" to \"http://localhost:1410/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the authentification for the R session using the credentials from the Google project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appID <- \"\" # Insert your own app Id here\n",
    "appSecret <- \"\" # insert your own app Secret here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the following line, there will be a prompt in the console asking you to save the access token in a file\n",
    "select \"No\" by entering 2 in the console and hitting enter.\n",
    "Afterwards, a browser window should open, prompting you to log in with your google account\n",
    "After logging in, you can close the browser and come back to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_oauth(appID,appSecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Sometimes, the authentification stops working (HTTP Failure: 401). If that happens, your authentification token has expired and you need to sign into your google account once more. To do that, simply go to the working directory, delete the file \".httr-oauth\", rerun the authentification command in R and a browser window with your google account login screen should open. You need to log in and close the browser afterwards. To prevent it from happening in the future, do **NOT** save your token in an object (press 2: \"No\" in the command line dialogue for the authentification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need the video ID of the youtube video(s) in question.\n",
    "We can find it by navigating to the Video in our webbrowser, and simply\n",
    "copying the last string of the URL that comes after the part \"?v=\"\n",
    "\n",
    "As an example, lets take the song \"Baby\" by Justin Bieber.\n",
    "You can find it on youtube here: https://www.youtube.com/watch?v=kffacxfA7G4\n",
    "\n",
    "So the necessary video ID would be \"kffacxfA7G4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the video ID in a variable\n",
    "VidID <- \"kffacxfA7G4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_results value between 20 and 100 to scrape an excerpt of the comments\n",
    "Comments <- get_comment_threads(c(video_id=VidID), max_results = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE your run this command, do a test run and see how quick you can fetch a few hundred comments\n",
    "# extrapolate from that whether you can safely fetch all comments at once or if you have to split\n",
    "# it up into smaller chunks.\n",
    "\n",
    "# Not recommended for the Justin Bieber Video (4.5 Million comments)\n",
    "\n",
    "# to extract all comments\n",
    "Comments <- get_all_comments(\"ENTER YOUR VIDEO ID HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Your dataset might contain less comments than are displayed as total comments on YouTube. This is because the tuber package only scrapes up to five _replies_ to each comment. If a comment has more than five replies, all subsequent replies will not be extracted by the tuber package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data effectively, we will have to format the data. Formatting will include:\n",
    "\n",
    "- Extract Emojis from comments and format them in human-readable and R-friendly formats\n",
    "- Extracting URLs from comments\n",
    "- Handling special characters in comments\n",
    "- Properly formatting timestamps into a format usable by R\n",
    "\n",
    "To this end, we wrote a custom function for formatting the data. This function is still work in progress and and this point far from computationally efficient, but it might nonetheless help you to bring the comments into a format you can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_parse <- function(x){\n",
    "        \n",
    "        #### We need to check first whether the data has 15 columns (Videos for which user has mod rights) or less (regular public videos)\n",
    "        \n",
    "        if (dim(x)[2] > 12) {\n",
    "                \n",
    "                # only keeping relevant columns\n",
    "                x <- x[,c(1,7,10,11,12,13,14)]\n",
    "                \n",
    "                # Converting dataframe columns to proper classes\n",
    "                x[,1] <- as.factor(x[,1])\n",
    "                x[,2] <- as.character(x[,2])\n",
    "                x[,3] <- as.numeric(x[,3])\n",
    "                x[,6] <- as.character(x[,6])\n",
    "                x[,7] <- as.character(x[,7])\n",
    "                \n",
    "                # converting timestamps into proper date-time objects\n",
    "                Published <- unlist(lapply(as.character(x[,4]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,4] <- as.POSIXct(Published, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                Updated <- unlist(lapply(as.character(x[,5]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,5] <- as.POSIXct(Updated, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                \n",
    "                #### Emoji\n",
    "                \n",
    "                ## we need a function to transfer Emoji Names to CamelCase (taken from: )\n",
    "                simpleCap <- function(x) {\n",
    "                        s <- strsplit(x, \" \")[[1]]\n",
    "                        paste(toupper(substring(s, 1,1)), substring(s, 2),\n",
    "                              sep=\"\", collapse=\" \")\n",
    "                }\n",
    "                \n",
    "                ## We need a function to detect and replace EMOJI in the fulltext comments\n",
    "                \n",
    "                ReplaceEM <- function(x) {\n",
    "                        \n",
    "                        \n",
    "                        # Setup: importing emoticon List\n",
    "                        EmoticonList <- jis\n",
    "                        \n",
    "                        ListedEmojis <- as.list(jis[,4])\n",
    "                        CamelCaseEmojis <- lapply(jis$name,simpleCap)\n",
    "                        CollapsedEmojis <- lapply(CamelCaseEmojis,function(x){gsub(\" \",\"\",x,fixed=TRUE)})\n",
    "                        EmoticonList[,4]$name <- unlist(CollapsedEmojis)\n",
    "                        \n",
    "                        # order the list by the length of the string to avoid partial matching of shorter strings\n",
    "                        EmoticonList <- EmoticonList[rev(order(nchar(jis$emoji))),]\n",
    "                        \n",
    "                        # Setup: We need to assign x to a new variable so we can save the progress in the for loop\n",
    "                        New <- x\n",
    "                        \n",
    "                        # rm_default throws a useless warning on each iteration that we can ignore\n",
    "                        oldw <- getOption(\"warn\")\n",
    "                        options(warn = -1)\n",
    "                        \n",
    "                        # cycle through the list and replace everything\n",
    "                        # we have to add clean = FALSE and trim = FALSE to not delete whitespaces that are part of the pattern.\n",
    "                        \n",
    "                        for (i in 1:dim(EmoticonList)[1]){\n",
    "                                \n",
    "                                New <- rm_default(New, pattern=EmoticonList[i,3],replacement= paste0(\"EMOJI_\", EmoticonList[i,4]$name, \" \"), fixed = TRUE, clean = FALSE, trim = FALSE)\n",
    "                                \n",
    "                        }\n",
    "                        \n",
    "                        # turning warnings back on\n",
    "                        options(warn = oldw)\n",
    "                        \n",
    "                        # output result\n",
    "                        return(New)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Creating a Text column where Emojis are replaced by their textual descriptions\n",
    "                \n",
    "                TextEmoRep <- ReplaceEM(x[,2])\n",
    "                \n",
    "                # Creating a text column where Emojis are deleted\n",
    "\n",
    "                TextEmoDel <- emo::ji_replace_all(x[,2],\"\")\n",
    "                \n",
    "                # Creating a Column listing only the textual descriptions of Emojis per message\n",
    "                \n",
    "                ExtractEM <- function(x){\n",
    "                        \n",
    "                        SpacerInsert <- gsub(\" \",\"[{[SpAC0R]}]\", x)\n",
    "                        ExtractEmoji <- rm_between(SpacerInsert,\"EMOJI_\",\"[{[SpAC0R]}]\",fixed=TRUE,extract = TRUE, clean= FALSE,trim=FALSE,include.markers = TRUE)\n",
    "                        UnlistEmoji <- unlist(ExtractEmoji)\n",
    "                        DeleteSpacer <- sapply(UnlistEmoji,function(x){gsub(\"[{[SpAC0R]}]\",\" \",x,fixed=T)})\n",
    "                        names(DeleteSpacer) <- NULL\n",
    "                        \n",
    "                        Emoji <-paste0(DeleteSpacer,collapse=\"\")\n",
    "                        return(Emoji)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Extracting and renaming the Emojis\n",
    "                Emoji <- sapply(TextEmoRep,ExtractEM)\n",
    "                \n",
    "                #### Links\n",
    "                \n",
    "                # Extracting links from comments\n",
    "                \n",
    "                Links <- qdapRegex::rm_url(x[,2], extract = TRUE)\n",
    "                Links <- I(Links)\n",
    "                \n",
    "                #### Combining it into one dataframe\n",
    "\n",
    "                a <- cbind.data.frame(x[,1],Emoji)\n",
    "                \n",
    "                df <- cbind.data.frame(x[,1],x[,2],TextEmoRep,TextEmoDel,Emoji,x[,3],Links,x[,4],x[,5],x[,6],x[,7])\n",
    "                names(df) <- c(\"Author\",\"Text\",\"TextEmojiReplaced\",\"TextEmojiDeleted\",\"Emoji\",\"LikeCount\",\"URL\",\"Published\",\"Updated\",\"CommentID\",\"ParentID\")\n",
    "                row.names(df) <- NULL\n",
    "                \n",
    "                \n",
    "        }\n",
    "        \n",
    "        else if(dim(x)[2] == 12){\n",
    "                \n",
    "                # only keeping relevant columns\n",
    "                x <- x[,c(1,7,10,11,12)]\n",
    "                \n",
    "                # Converting dataframe columns to proper classes\n",
    "                x[,1] <- as.factor(x[,1])\n",
    "                x[,2] <- as.character(x[,2])\n",
    "                x[,3] <- as.numeric(x[,3])\n",
    "                \n",
    "                # converting timestamps into proper date-time objects\n",
    "                Published <- unlist(lapply(as.character(x[,4]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,4] <- as.POSIXct(Published, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                Updated <- unlist(lapply(as.character(x[,5]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,5] <- as.POSIXct(Updated, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                \n",
    "                #### Emoji\n",
    "                \n",
    "                ## we need a function to transfer Emoji Names to CamelCase (taken from: )\n",
    "                simpleCap <- function(x) {\n",
    "                        s <- strsplit(x, \" \")[[1]]\n",
    "                        paste(toupper(substring(s, 1,1)), substring(s, 2),\n",
    "                              sep=\"\", collapse=\" \")\n",
    "                }\n",
    "                \n",
    "                ## We need a function to detect and replace EMOJI in the fulltext comments\n",
    "                \n",
    "                ReplaceEM <- function(x) {\n",
    "                        \n",
    "                        \n",
    "                        # Setup: importing emoticon List\n",
    "                        EmoticonList <- jis\n",
    "                        \n",
    "                        ListedEmojis <- as.list(jis[,4])\n",
    "                        CamelCaseEmojis <- lapply(jis$name,simpleCap)\n",
    "                        CollapsedEmojis <- lapply(CamelCaseEmojis,function(x){gsub(\" \",\"\",x,fixed=TRUE)})\n",
    "                        EmoticonList[,4]$name <- unlist(CollapsedEmojis)\n",
    "                        \n",
    "                        # order the list by the length of the string to avoid partial matching of shorter strings\n",
    "                        EmoticonList <- EmoticonList[rev(order(nchar(jis$emoji))),]\n",
    "                        \n",
    "                        # Setup: We need to assign x to a new variable so we can save the progress in the for loop\n",
    "                        New <- x\n",
    "                        \n",
    "                        # rm_default throws a useless warning on each iteration that we can ignore\n",
    "                        oldw <- getOption(\"warn\")\n",
    "                        options(warn = -1)\n",
    "                        \n",
    "                        # cycle through the list and replace everything\n",
    "                        # we have to add clean = FALSE and trim = FALSE to not delete whitespaces that are part of the pattern.\n",
    "                        \n",
    "                        for (i in 1:dim(EmoticonList)[1]){\n",
    "                                \n",
    "                                New <- rm_default(New, pattern=EmoticonList[i,3],replacement= paste0(\"EMOJI_\", EmoticonList[i,4]$name, \" \"), fixed = TRUE, clean = FALSE, trim = FALSE)\n",
    "                                \n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        # turning warnings back on\n",
    "                        options(warn = oldw)\n",
    "                        \n",
    "                        # output result\n",
    "                        return(New)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Creating a Text column where Emojis are replaced by their textual descriptions\n",
    "                \n",
    "                TextEmoRep <- ReplaceEM(x[,2])\n",
    "                \n",
    "                # Creating a text column where Emojis are deleted\n",
    "                \n",
    "                TextEmoDel <- emo::ji_replace_all(x[,2],\"\")\n",
    "                \n",
    "                # Creating a Column listing only the textual descriptions of Emojis per message\n",
    "                \n",
    "                ExtractEM <- function(x){\n",
    "                        \n",
    "                        SpacerInsert <- gsub(\" \",\"[{[SpAC0R]}]\", x)\n",
    "                        ExtractEmoji <- rm_between(SpacerInsert,\"EMOJI_\",\"[{[SpAC0R]}]\",fixed=TRUE,extract = TRUE, clean= FALSE,trim=FALSE,include.markers = TRUE)\n",
    "                        UnlistEmoji <- unlist(ExtractEmoji)\n",
    "                        DeleteSpacer <- sapply(UnlistEmoji,function(x){gsub(\"[{[SpAC0R]}]\",\" \",x,fixed=T)})\n",
    "                        names(DeleteSpacer) <- NULL\n",
    "                        \n",
    "                        Emoji <-paste0(DeleteSpacer,collapse=\"\")\n",
    "                        return(Emoji)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Extracting and renaming the Emojis\n",
    "                Emoji <- sapply(TextEmoRep,ExtractEM)\n",
    "                \n",
    "                \n",
    "                #### Links\n",
    "                \n",
    "                # Extracting links from comments\n",
    "                \n",
    "                Links <- qdapRegex::rm_url(x[,2], extract = TRUE)\n",
    "                Links <- I(Links)\n",
    "                \n",
    "                #### Combining it into one dataframe\n",
    "                \n",
    "                df <- cbind.data.frame(x[,1],x[,2],TextEmoRep,TextEmoDel,Emoji,x[,3],Links,x[,4],x[,5])\n",
    "                names(df) <- c(\"Author\",\"Text\",\"TextEmojiReplaced\",\"TextEmojiDeleted\",\"Emoji\",\"LikeCount\",\"URL\",\"Published\",\"Updated\")\n",
    "                row.names(df) <- NULL\n",
    "                \n",
    "                \n",
    "        }\n",
    "        \n",
    "        \n",
    "        #### Returning dataframe\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply use this function on the comments dataframe that we extracted to get a new dataframe that is\n",
    "formatted nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function to format the \"Comments\" dataframe \n",
    "FormattedComments <- yt_parse(Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 10 formatted comments\n",
    "head(FormattedComments,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: The textual analysis is adapted from: https://docs.quanteda.io/articles/pkgdown/examples/plotting.html\n",
    "\n",
    "For the textual analysis, we will use the column of our formatted dataframe that does **not** contain any Emojis in any format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diyplaying first 10 elements of the column we will use for the text analysis\n",
    "head(FormattedComments$TextEmojiDeleted,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will tokenize the the comments (e.g. splitting them into single words). At the same time, we will remove numbers, punctuation,\n",
    "seperators, symbols, hyphens and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizing the comments (splitting them into single words and signs)\n",
    "toks <- tokens(char_tolower(FormattedComments$TextEmojiDeleted),\n",
    "               remove_numbers = TRUE,\n",
    "               remove_punct = TRUE,\n",
    "               remove_separators = TRUE,\n",
    "               remove_symbols = TRUE,\n",
    "               remove_hyphens = TRUE,\n",
    "               remove_url = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first 10 tokenized comments\n",
    "toks[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a document frequency matrix (https://en.wikipedia.org/wiki/Document-term_matrix .\n",
    "In our case, a document is a comment, so  put simply, this matrix counts how often each of the words contained in any comment is appearing in every single comment. We do this while removing Stopwords (https://en.wikipedia.org/wiki/Stop_words), which can negatively influence the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a document frequency matrix while removing Stopwords\n",
    "# Stopwords are very frequent words that occur in all texts (e.g. \"a\",\"but\",\"it\")\n",
    "commentsDfm <- dfm(toks, remove = quanteda::stopwords(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the frequency of terms in the documents\n",
    "TermFreq <- textstat_frequency(commentsDfm)\n",
    "TermFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by total occurances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we want to use the DFM we created to visualize the most freqeuent tokens across all comments. First, we order\n",
    "the tokens, then we plot their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by reverse frequency order\n",
    "TermFreq$feature <- with(TermFreq, reorder(feature, -frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the x most common tokens (you can change x to suit your needs)\n",
    "x <- 25\n",
    "\n",
    "ggplot(TermFreq[1:x], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview might be biased because it just counts the total sum of accurances of each tokens across all comments. It might thus be that there is only one person spamming the same word hundreds of times in a single comment. To mitigate this, we will also count the number of comments that contain each token at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the x tokens that are used in the highest number of comments (you can change x to suit your needs)\n",
    "x <- 25\n",
    "\n",
    "ggplot(TermFreq[1:x], aes(x = feature, y = docfreq)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the most frequent terms, we might want to exclude certain terms that are not indicative for the comments (e.g. the word \"video\")or certain words that are used by spammers (e.g. \"viagra\"). Which words to exclude is the individual decision of each researcher. You should be carefull with designing this list for your needs and be transparent which words you excluded and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Stopword List\n",
    "CustomStops <- c(\"video\",\"wow\",\"million\",\"much\",\"oh\",\"zouhir\",\"bahoui\")\n",
    "\n",
    "# This is just an example, you should carefully create your own list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create another document-frequency matrix that excludes the Custom Stopwords that we just defined, and then rerun the code above to update our results\n",
    "commentsDfm <- dfm(toks, remove = c(quanteda::stopwords(\"english\"),CustomStops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerunning steps from above in one cell with new DFM (excluding custom stop words)\n",
    "TermFreq <- textstat_frequency(commentsDfm)\n",
    "TermFreq$feature <- with(TermFreq, reorder(feature, -frequency))\n",
    "x <- 25\n",
    "ggplot(TermFreq[1:x], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the DFM to very easily create a wordcloud to visualize the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for most frequently used Terms\n",
    "set.seed(12345)\n",
    "textplot_wordcloud(dfm_select(commentsDfm, min_nchar=3),\n",
    "                   random_order=FALSE,\n",
    "                   max_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Sentiment Analysis (https://en.wikipedia.org/wiki/Sentiment_analysis) on the comments to get an intuition about peoples opinions towards the content of the videos. Sentiment analysis works by comparing the tokens in each comment to a dictionary of words with an attached sentiment rating. For example, the word \"fuck\" would have a negative sentiment rating in the dictionary while the word \"love\" would have a positive sentiment rating. If we add the sentiment scores of all tokens in a given comment, we get an overall sentiment of that comment.\n",
    "\n",
    "Of course, the results dependend on the kind of dictionary that is used. We chose the AFINN dictionary (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), because it is\n",
    "based on the language in microblogs and thus might capture the slang/tone of online comments better than other dictionaries.\n",
    "\n",
    "However, the choice of the dictionary is up to the individual researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting sentiment scores per comment\n",
    "CommentSentiment <- syuzhet::get_sentiment(FormattedComments$TextEmojiDeleted, method = \"afinn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some summary statistics and a basic vizualisation of sentiment across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "summary(CommentSentiment)\n",
    "boxplot(CommentSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually inspect comments that have extreme ratings as it´s always good to check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying comments with a sentiment score below x\n",
    "x  <- -3\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment < x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disyplaying comments with a sentiment score above x\n",
    "x <- 5\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment > x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying most negative/positive comment\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment == min(CommentSentiment)]\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment == max(CommentSentiment)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Comment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better overview, we can also display the total amount of positive, negative and neutral comments. To this end,\n",
    "we need to create a new dataframe as a crutch to categorize the comments first.\n",
    "\n",
    "We can also display the total distribution of sentiment in comments and overlay the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building helper Frame\n",
    "Desc <- CommentSentiment\n",
    "Desc[Desc > 0] <- \"positive\"\n",
    "Desc[Desc < 0] <- \"negative\"\n",
    "Desc[Desc == 0] <- \"neutral\"\n",
    "df <- data.frame(FormattedComments$TextEmojiDeleted,CommentSentiment,Desc)\n",
    "colnames(df) <- c(\"Comment\",\"Sentiment\",\"Desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the amount of positive, negative and neutral comments\n",
    "ggplot(data=df, aes(x=Desc, fill = Desc)) +\n",
    "        geom_bar(stat='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of comment sentiments\n",
    "ggplot(df, aes(x=Sentiment)) +\n",
    "        geom_histogram(binwidth = 1) +\n",
    "        geom_vline(aes(xintercept=mean(Sentiment)),\n",
    "           color=\"black\", linetype=\"dashed\", size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only analyzed the text of the comments but we can also analyze the used Emojis in the comments.\n",
    "To this end, we first format NA values correctly for comments that do not contain any Emojis.\n",
    "Second, we tokenize the Emojis just as we did with the text strings. Then, we build an EmojiFreqeuncy Matrix\n",
    "that counts how often each Emojis is contained in every comment. Lastly, we visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting NA´s correctly\n",
    "FormattedComments$Emoji[FormattedComments$Emoji == \"NA\"] <- NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing spaces at the end of the string\n",
    "FormattedComments$Emoji <- substr(FormattedComments$Emoji, 1, nchar(FormattedComments$Emoji)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokinizing\n",
    "EmojiToks <- tokens(FormattedComments$Emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Emojis in the first 10 comments\n",
    "EmojiToks[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build an Emoji Frequency Matrix, excluding \"NA\" as a term\n",
    "EmojiDfm <- dfm(EmojiToks,remove = \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the frequency of Emojis in the documents\n",
    "EmojiFreq <- textstat_frequency(EmojiDfm)\n",
    "EmojiFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get a more sparse overview of the x top Emojis in the comments\n",
    "x = 20\n",
    "topfeatures(EmojiDfm,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by total occurances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the EmojiFrequency Matrix, we can plot the most frequently occuring Emojis across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by reverse frequency order\n",
    "EmojiFreq$feature <- with(EmojiFreq, reorder(feature, -frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "ggplot(EmojiFreq, aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by number of comments containing the Emoji at least once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview might be biased because it just counts the total sum of accurances of each Emoji across all comments. It might thus be that there is only one person spamming the same Emoji hundreds of times in a single comment. To mitigate this, we will also count the number of comments that contain each Emoji at least once.\n",
    "\n",
    "Basically, we´re counting the number of comments that do contain the Emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by reverse document frequency order\n",
    "EmojiFreq$feature <- with(EmojiFreq, reorder(feature, -docfreq))\n",
    "\n",
    "#plotting\n",
    "ggplot(EmojiFreq, aes(x = feature, y = docfreq)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Emojis (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like text (and arguably even more so), Emojis are used to confer emotions and opinion. For this reason, we´re trying here for an explorative sentiment analysis using the Emojis in the comments. Just as for the sentences,we thus need a dicitionary that maps Emojis to Sentiments. Unfortunately, there so far seems to be only one sentiment dictionary for  the most commonly used 734 Emojis (http://kt.ijs.si/data/Emoji_sentiment_ranking/) . Thus, we currently do not have the possibility to check the results with different dictionaries and cannot inlcude all Emojis in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing emojis dictionary (We only get 734 different Emojis but thats the best data we have on Emoji Sentiment)\n",
    "EmojiSentiments <- lexicon::emojis_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first ten rows of the Emoji Sentiment dictionary\n",
    "EmojiSentiments[1:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to match the sentiment scores to our codings of the emojis and create a quanteda dictionary object\n",
    "EmojiNames <- paste0(\"emoji_\",gsub(\" \",\"\",EmojiSentiments$name))\n",
    "EmojiSentiment <- cbind.data.frame(EmojiNames,EmojiSentiments$sentiment,EmojiSentiments$polarity)\n",
    "names(EmojiSentiment) <- c(\"word\",\"sentiment\",\"valence\")\n",
    "EmojiSentDict <- as.dictionary(EmojiSentiment[,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the Emoji-only column in our formatted dataframe\n",
    "EmojiToks <- tokens(tolower(FormattedComments$Emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now replace the emojis in the dictionary with the corresponding sentiment scores\n",
    "EmojiToksSent <- tokens_replace(EmojiToks,EmojiSentDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many Emoji we can cover with sentiment scores\n",
    "A <- unlist(EmojiToksSent)\n",
    "names(A) <- NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mapping the Emojis in our dataset to the sentiment scores in the dictionary, we can check how many Emojis we have in total in our dataset and how many of those we are getting sentiment mappings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total Emoji\n",
    "B <- A[A!=\"NA\"]\n",
    "length(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check\n",
    "\n",
    "# Number of Emoji that couldn´t be replaced\n",
    "length(grep(\"emoji_\",B))\n",
    "\n",
    "# number of Emoji that could be replaced\n",
    "length(grep(\"0.\",B))\n",
    "\n",
    "# Percentage of Emoji that couldn´t be replaced\n",
    "length(grep(\"emoji_\",B))/length(B)\n",
    "\n",
    "# Percentage of Emoji that could be replaced\n",
    "length(grep(\"0.\",B))/length(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add sentiments for Emojis within the same comment to get an overall comment sentiment based on Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing sentiment scores for comments based on Emoji\n",
    "\n",
    "# only keeping the replaced sentiment scores for th Emoji vector\n",
    "D <- tokens_select(EmojiToksSent,EmojiSentiment$sentiment,\"keep\")\n",
    "D <- as.list(D)\n",
    "\n",
    "# function to add sentiment scores of Emojis per comment\n",
    "AddEmojiSentiments <- function(x){\n",
    "        \n",
    "        x <- sum(as.numeric(as.character(x)))\n",
    "        return(x)\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "AdditiveEmojiSentiment <- lapply(D,AddEmojiSentiments)\n",
    "AdditiveEmojiSentiment[AdditiveEmojiSentiment == 0] <- NA\n",
    "AdditiveEmojiSentiment <- unlist(AdditiveEmojiSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting histogram for distribution of Emoji Sentiment Scores\n",
    "hist(AdditiveEmojiSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation between Emoji sentiment score and text Sentiment Score\n",
    "cor(CommentSentiment,AdditiveEmojiSentiment,use=\"complete.obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting the relationship\n",
    "plot(CommentSentiment,AdditiveEmojiSentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
