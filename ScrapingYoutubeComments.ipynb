{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scraping and Analyzing Youtube Comments in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will give a detailed walktrough of how to use the Tuber package (https://cran.r-project.org/web/packages/tuber/tuber.pdf) to extract youtube comments, format them, and give some basic examples for analysis of text and contained emojis.\n",
    "\n",
    "The script is part of an ongoing research project (https://www.researchgate.net/project/Methods-and-Tools-for-Automatic-Sampling-and-Analysis-of-YouTube-Comments) and will be subject to change. If you use substantive parts of this script as part of your own research, please cite it in the following way:\n",
    "\n",
    "**(INSERT CITATION HERE)**\n",
    "\n",
    "This notebook is structured in five parts:\n",
    "\n",
    "1) [Setting up your local R environment](#Setup)\n",
    "\n",
    "2) [Create and athenticate an account for the Youtube API](#Authentification)\n",
    "\n",
    "3) [Extract data from youtute](#Extraction)\n",
    "\n",
    "3) [Import Data to this Notebook to do exemplary analysis online, if you don't want a local version](#Import)\n",
    "\n",
    "4) [Extract, process and analyse text and emojis from comment data](#Analysis)\n",
    "\n",
    "You can thus either run an exemplary analysis in the browser without creating your own local files (simply start with step 3). Or setup our own local data collection script (Step 1 & 2). You can also download this notebook as an R script (File -> Download as -> R(.r)) or Markdown file (File -> Download as -> Makrdown(.md)) and use it locally to adapt our analysis with other video URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Setup'></a> Setting up Local Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to set up our R environment correctly so we are able to use the tuber package and all other\n",
    "necessary packages. For this, open a new Script in R Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all objects from the current global environment\n",
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set the correct working directory. It´s best if all files that you want to use troughout your analysis are in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a working directory in a GUI window\n",
    "dir <- choose.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing working directory\n",
    "setwd(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove directory string from global environment\n",
    "rm(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we´re setting options so that text strings are not automatically detected as factor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options parameter so that textstrings are not interpreted as factor variables\n",
    "options(stringsAsFactors = FALSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a list of packages that we need to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of necessary packages for data extraction and analysis\n",
    "packages <- c(\"devtools\",\n",
    "              \"tm\",\n",
    "              \"quanteda\",\n",
    "              \"tuber\",\n",
    "              \"qdapRegex\",\n",
    "              \"rlang\",\n",
    "              \"purrr\",\n",
    "              \"ggplot2\",\n",
    "              \"syuzhet\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We than install the packages and attach them in our R session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all packages in list\n",
    "install.packages(packages,repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attaching all packages in list\n",
    "lapply(packages, library, character.only = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another package that we need to install from GitHub because it is not yet on CRAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the emo package from github (not on CRAN yet)\n",
    "devtools::install_github(\"hadley/emo\")\n",
    "library(emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can check if all the packages we need have been installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if packages have been properly attached\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Authentification'></a> Authentication for Youtube API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to data from Youtube, we have to use the Youtube API (https://developers.google.com/youtube/v3/).\n",
    "To do this, we need a token that identifies us when using the API so Youtube can be sure\n",
    "that their Terms and Conditions are respected (e.g. there is a certain limit how much data you can access in\n",
    "a given timeframe). We´ll go about this step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you do not have a google account that you are willing to use for this project, you have to create a new one here https://accounts.google.com/signup/v2/webcreateaccount?hl=en-GB&flowName=GlifWebSignIn&flowEntry=SignUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With your google account, you have to create a \"Google Project\" and configure it correctly so you can get the right credentials to get access to the data with the tuber package. In case that you need help, we have set up a video showcasing and describing the process. You can find it here: https://www.youtube.com/watch?v=qLrNq0jWH84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the credentials of the project-account to authenticate your R-session. By doing this, you allow R access to all data on youtube,that you would be able to see if you went to the site logged in with this account. Be carefull to **NOT** share these credentials with anyone else you don´t want to be able to log into this account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the authentification for the R session using the credentials from the Google project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The credentials below are from an account that we specifically set up to scrape youtube comments.\n",
    "# You have to use your own credentials when you make calls to the API. \n",
    "\n",
    "appID <- \"\" # Insert your own app Id here\n",
    "appSecret <- \"\" # insert your own app Secret here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon running the following line, there will be a prompt in the console asking you to save the access token in a file. Select \"No\" by entering 2 in the console and hitting enter. Afterwards, a browser window should open, prompting you to log in with your google account. After logging in, you can close the browser and come back to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_oauth(appID,appSecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Extraction'></a>Extracting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need the video ID of the youtube video(s) in question.\n",
    "We can find it by navigating to the Video in our webbrowser, and simply\n",
    "copying the last string of the URL that comes after the part \"?v=\"\n",
    "\n",
    "As an example, lets take the song \"Baby\" by Justin Bieber.\n",
    "You can find it on youtube here: https://www.youtube.com/watch?v=kffacxfA7G4\n",
    "\n",
    "So the necessary video ID would be \"kffacxfA7G4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the video ID in a variable\n",
    "VidID <- \"kffacxfA7G4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_results value between 20 and 100 to scrape an excerpt of the comments\n",
    "Comments <- get_comment_threads(c(video_id=VidID), max_results = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE your run this command, do a test run and see how quickly you can fetch a few hundred comments\n",
    "# extrapolate from that whether you can safely fetch all comments at once or if you have to split\n",
    "# it up into smaller chunks.\n",
    "\n",
    "# Not recommended for the Justin Bieber Video (4.5 Million comments)\n",
    "\n",
    "# to extract all comments\n",
    "Comments <- get_all_comments(c(video_id=\"ENTER YOUR VIDEO ID HERE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Your dataset might contain less comments than are displayed as total comments on YouTube. This is because the tuber package only scrapes up to five _replies_ to each comment. If a comment has more than five replies, all subsequent replies will not be extracted by the tuber package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Import'></a> Importing Prepared Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is not possible yet to scrape comments from this session directly, we downloaded all comments (31.08.2018) from this video: https://www.youtube.com/watch?v=DcJFdCmN98s so we can showcase the data formatting and analysis interactively in this Browser session.\n",
    "\n",
    "**If you downloaded this as an R script or markdown and use it locally, you can ignore this section as it will not work on your local machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set options parameter so that textstrings are not interpreted as factor variables\n",
    "options(stringsAsFactors = FALSE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-downloaded Comments Datafile\n",
    "load(\"DayumComments.Rdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disyplaing first 10 rows of datafile\n",
    "head(Comments,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of necessary packages for data analysis\n",
    "packages <- c(\"devtools\",\n",
    "              \"tm\",\n",
    "              \"quanteda\",\n",
    "              \"tuber\",\n",
    "              \"qdapRegex\",\n",
    "              \"rlang\",\n",
    "              \"purrr\",\n",
    "              \"ggplot2\",\n",
    "              \"syuzhet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attaching all packages in list\n",
    "lapply(packages, library, character.only = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the emo package from github (not on CRAN yet)\n",
    "devtools::install_github(\"hadley/emo\")\n",
    "library(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the emoGG package from github (not on CRAN yet): Displays Emoji in ggplot objects\n",
    "devtools::install_github(\"dill/emoGG\")\n",
    "library(emoGG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='Analysis'></a> Formatting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data effectively, we will have to format the data. Formatting will include:\n",
    "\n",
    "- Extract Emojis from comments and format them in human-readable and R-friendly formats\n",
    "- Extracting URLs from comments\n",
    "- Handling special characters in comments\n",
    "- Properly formatting timestamps into a format usable by R\n",
    "\n",
    "To this end, we wrote a custom function for formatting the data. This function is still work in progress and and this point far from computationally efficient, but it might nonetheless help you to bring the comments into a format you can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_parse <- function(x){\n",
    "        \n",
    "        #### We need to first check first the dataframe has 15 columns (videos with replies to comments) or fewer (videos without replies to comments)\n",
    "        \n",
    "        if (dim(x)[2] > 13) {\n",
    "                \n",
    "                # only keep the relevant columns\n",
    "                x <- x[,c(1,7,10,11,12,13,14)]\n",
    "                \n",
    "                # Convert dataframe columns to proper types\n",
    "                x[,1] <- as.factor(x[,1])\n",
    "                x[,2] <- as.character(x[,2])\n",
    "                x[,3] <- as.numeric(x[,3])\n",
    "                x[,6] <- as.character(x[,6])\n",
    "                x[,7] <- as.character(x[,7])\n",
    "                \n",
    "                # convert timestamps into proper date-time objects\n",
    "                Published <- unlist(lapply(as.character(x[,4]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,4] <- as.POSIXct(Published, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                Updated <- unlist(lapply(as.character(x[,5]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,5] <- as.POSIXct(Updated, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                \n",
    "                #### Emojis\n",
    "                \n",
    "                ## convert Emoji names to CamelCase (if you want to learn (more) about CamelCase: https://en.wikipedia.org/wiki/Camel_case)\n",
    "                simpleCap <- function(x) {\n",
    "                        s <- strsplit(x, \" \")[[1]]\n",
    "                        paste(toupper(substring(s, 1,1)), substring(s, 2),\n",
    "                              sep=\"\", collapse=\" \")\n",
    "                }\n",
    "                \n",
    "                ## detect and replace Emojis in the comments\n",
    "                \n",
    "                ReplaceEM <- function(x) {\n",
    "                        \n",
    "                        \n",
    "                        # Setup: import Emoji List\n",
    "                        EmoticonList <- jis\n",
    "                        \n",
    "                        ListedEmojis <- as.list(jis[,4])\n",
    "                        CamelCaseEmojis <- lapply(jis$name,simpleCap)\n",
    "                        CollapsedEmojis <- lapply(CamelCaseEmojis,function(x){gsub(\" \",\"\",x,fixed=TRUE)})\n",
    "                        EmoticonList[,4]$name <- unlist(CollapsedEmojis)\n",
    "                        \n",
    "                        # order the list by the length of the string to avoid partial matching of shorter strings\n",
    "                        EmoticonList <- EmoticonList[rev(order(nchar(jis$emoji))),]\n",
    "                        \n",
    "                        # Setup: We need to assign x to a new variable so we can save the progress in the for-loop (see below)\n",
    "                        New <- x\n",
    "                        \n",
    "                        # rm_default throws a useless warning on each iteration that we can ignore\n",
    "                        oldw <- getOption(\"warn\")\n",
    "                        options(warn = -1)\n",
    "                        \n",
    "                        # cycle through the list and replace everything\n",
    "                        # we have to add clean = FALSE and trim = FALSE to avoid deleting whitespaces that are part of the pattern\n",
    "                        \n",
    "                        for (i in 1:dim(EmoticonList)[1]){\n",
    "                                \n",
    "                                New <- rm_default(New, pattern=EmoticonList[i,3],replacement= paste0(\"EMOJI_\", EmoticonList[i,4]$name, \" \"), fixed = TRUE, clean = FALSE, trim = FALSE)\n",
    "                                \n",
    "                        }\n",
    "                        \n",
    "                        # turn warning messages back on\n",
    "                        options(warn = oldw)\n",
    "                        \n",
    "                        # output result\n",
    "                        return(New)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Create a text column in which Emojis are replaced by their textual descriptions\n",
    "                \n",
    "                TextEmoRep <- ReplaceEM(x[,2])\n",
    "                \n",
    "                # Create a text column in which Emojis are deleted\n",
    "\n",
    "                TextEmoDel <- emo::ji_replace_all(x[,2],\"\")\n",
    "                \n",
    "                # Create a column with only the textual descriptions of Emojis for each comment\n",
    "                \n",
    "                ExtractEM <- function(x){\n",
    "                        \n",
    "                        SpacerInsert <- gsub(\" \",\"[{[SpAC0R]}]\", x)\n",
    "                        ExtractEmoji <- rm_between(SpacerInsert,\"EMOJI_\",\"[{[SpAC0R]}]\",fixed=TRUE,extract = TRUE, clean= FALSE,trim=FALSE,include.markers = TRUE)\n",
    "                        UnlistEmoji <- unlist(ExtractEmoji)\n",
    "                        DeleteSpacer <- sapply(UnlistEmoji,function(x){gsub(\"[{[SpAC0R]}]\",\" \",x,fixed=T)})\n",
    "                        names(DeleteSpacer) <- NULL\n",
    "                        \n",
    "                        Emoji <-paste0(DeleteSpacer,collapse=\"\")\n",
    "                        return(Emoji)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Extract and rename Emojis\n",
    "                Emoji <- sapply(TextEmoRep,ExtractEM)\n",
    "                \n",
    "                #### URLs\n",
    "                \n",
    "                # Extract URLs from comments\n",
    "                \n",
    "                Links <- qdapRegex::rm_url(x[,2], extract = TRUE)\n",
    "                Links <- I(Links)\n",
    "                \n",
    "                #### Combine everything into one dataframe\n",
    "\n",
    "                a <- cbind.data.frame(x[,1],Emoji)\n",
    "                \n",
    "                df <- cbind.data.frame(x[,1],x[,2],TextEmoRep,TextEmoDel,Emoji,x[,3],Links,x[,4],x[,5],x[,6],x[,7])\n",
    "                names(df) <- c(\"Author\",\"Text\",\"TextEmojiReplaced\",\"TextEmojiDeleted\",\"Emoji\",\"LikeCount\",\"URL\",\"Published\",\"Updated\",\"ModerationStatus\",\"CommentID\")\n",
    "                row.names(df) <- NULL\n",
    "                \n",
    "                \n",
    "        }\n",
    "        \n",
    "        else {\n",
    "                \n",
    "                # only keep relevant columns\n",
    "                x <- x[,c(1,7,10,11,12)]\n",
    "                \n",
    "                # Convert dataframe columns to proper types\n",
    "                x[,1] <- as.factor(x[,1])\n",
    "                x[,2] <- as.character(x[,2])\n",
    "                x[,3] <- as.numeric(x[,3])\n",
    "                \n",
    "                # convert timestamps into proper date-time objects\n",
    "                Published <- unlist(lapply(as.character(x[,4]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,4] <- as.POSIXct(Published, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                Updated <- unlist(lapply(as.character(x[,5]),function(x){paste(substr(x,1,10),substr(x,12,19),sep = \"-\")}))\n",
    "                x[,5] <- as.POSIXct(Updated, format =\"%Y-%m-%d-%H:%M:%S \", tz = \"UTC\")\n",
    "                \n",
    "                \n",
    "                #### Emoji\n",
    "                \n",
    "                ## convert Emoji names to CamelCase (What is CamelCase? https://en.wikipedia.org/wiki/Camel_case)\n",
    "                simpleCap <- function(x) {\n",
    "                        s <- strsplit(x, \" \")[[1]]\n",
    "                        paste(toupper(substring(s, 1,1)), substring(s, 2),\n",
    "                              sep=\"\", collapse=\" \")\n",
    "                }\n",
    "                \n",
    "                ## detect and replace Emojis in the comments\n",
    "                \n",
    "                ReplaceEM <- function(x) {\n",
    "                        \n",
    "                        \n",
    "                        # Setup: import Emoji List\n",
    "                        EmoticonList <- jis\n",
    "                        \n",
    "                        ListedEmojis <- as.list(jis[,4])\n",
    "                        CamelCaseEmojis <- lapply(jis$name,simpleCap)\n",
    "                        CollapsedEmojis <- lapply(CamelCaseEmojis,function(x){gsub(\" \",\"\",x,fixed=TRUE)})\n",
    "                        EmoticonList[,4]$name <- unlist(CollapsedEmojis)\n",
    "                        \n",
    "                        # order the list by the length of the string to avoid partial matching of shorter strings\n",
    "                        EmoticonList <- EmoticonList[rev(order(nchar(jis$emoji))),]\n",
    "                        \n",
    "                        # Setup: We need to assign x to a new variable so we can save the progress in the for-loop (see below)\n",
    "                        New <- x\n",
    "                        \n",
    "                        # rm_default throws a useless warning on each iteration that we can ignore\n",
    "                        oldw <- getOption(\"warn\")\n",
    "                        options(warn = -1)\n",
    "                        \n",
    "                        # cycle through the list and replace everything\n",
    "                        # we have to add clean = FALSE and trim = FALSE to avoid deleting whitespaces that are part of the pattern.\n",
    "                        \n",
    "                        for (i in 1:dim(EmoticonList)[1]){\n",
    "                                \n",
    "                                New <- rm_default(New, pattern=EmoticonList[i,3],replacement= paste0(\"EMOJI_\", EmoticonList[i,4]$name, \" \"), fixed = TRUE, clean = FALSE, trim = FALSE)\n",
    "                                \n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        # turn warnings back on\n",
    "                        options(warn = oldw)\n",
    "                        \n",
    "                        # output result\n",
    "                        return(New)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # create a text column in which Emojis are replaced by their textual descriptions\n",
    "                \n",
    "                TextEmoRep <- ReplaceEM(x[,2])\n",
    "                \n",
    "                # create a text column in which Emojis are deleted\n",
    "                \n",
    "                TextEmoDel <- emo::ji_replace_all(x[,2],\"\")\n",
    "                \n",
    "                # create a column with only the textual descriptions of Emojis for each comment\n",
    "                \n",
    "                ExtractEM <- function(x){\n",
    "                        \n",
    "                        SpacerInsert <- gsub(\" \",\"[{[SpAC0R]}]\", x)\n",
    "                        ExtractEmoji <- rm_between(SpacerInsert,\"EMOJI_\",\"[{[SpAC0R]}]\",fixed=TRUE,extract = TRUE, clean= FALSE,trim=FALSE,include.markers = TRUE)\n",
    "                        UnlistEmoji <- unlist(ExtractEmoji)\n",
    "                        DeleteSpacer <- sapply(UnlistEmoji,function(x){gsub(\"[{[SpAC0R]}]\",\" \",x,fixed=T)})\n",
    "                        names(DeleteSpacer) <- NULL\n",
    "                        \n",
    "                        Emoji <-paste0(DeleteSpacer,collapse=\"\")\n",
    "                        return(Emoji)\n",
    "                        \n",
    "                }\n",
    "                \n",
    "                # Extract and rename Emojis\n",
    "                Emoji <- sapply(TextEmoRep,ExtractEM)\n",
    "                \n",
    "                \n",
    "                #### URLs\n",
    "                \n",
    "                # Extract URLs from comments\n",
    "                \n",
    "                Links <- qdapRegex::rm_url(x[,2], extract = TRUE)\n",
    "                Links <- I(Links)\n",
    "                \n",
    "                #### Combine everything into one dataframe\n",
    "                \n",
    "                df <- cbind.data.frame(x[,1],x[,2],TextEmoRep,TextEmoDel,Emoji,x[,3],Links,x[,4],x[,5])\n",
    "                names(df) <- c(\"Author\",\"Text\",\"TextEmojiReplaced\",\"TextEmojiDeleted\",\"Emoji\",\"LikeCount\",\"URL\",\"Published\",\"Updated\")\n",
    "                row.names(df) <- NULL\n",
    "                \n",
    "                \n",
    "        }\n",
    "        \n",
    "        \n",
    "        #### Return dataframe\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply use this function on the comments dataframe that we extracted to get a new dataframe that is\n",
    "formatted nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function to format the \"Comments\" dataframe (may take a while, depending on the size of the Comment object)\n",
    "FormattedComments <- yt_parse(Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 10 formatted comments\n",
    "head(FormattedComments,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: The textual analysis is adapted from: https://docs.quanteda.io/articles/pkgdown/examples/plotting.html\n",
    "\n",
    "For the textual analysis, we will use the column of our formatted dataframe that does **not** contain any Emojis in any format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying first 10 elements of the column we will use for the text analysis\n",
    "head(FormattedComments$TextEmojiDeleted,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will tokenize the the comments (e.g. splitting them into single words). At the same time, we will remove numbers, punctuation,\n",
    "seperators, symbols, hyphens and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizing the comments (splitting them into single words and signs)\n",
    "toks <- tokens(char_tolower(FormattedComments$TextEmojiDeleted),\n",
    "               remove_numbers = TRUE,\n",
    "               remove_punct = TRUE,\n",
    "               remove_separators = TRUE,\n",
    "               remove_symbols = TRUE,\n",
    "               remove_hyphens = TRUE,\n",
    "               remove_url = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first 10 tokenized comments\n",
    "toks[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a document frequency matrix (https://en.wikipedia.org/wiki/Document-term_matrix .\n",
    "In our case, a document is a comment, so  put simply, this matrix counts how often each of the words contained in any comment is appearing in every single comment. We do this while removing Stopwords (https://en.wikipedia.org/wiki/Stop_words), which can negatively influence the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a document frequency matrix while removing Stopwords\n",
    "# Stopwords are very frequent words that occur in all texts (e.g. \"a\",\"but\",\"it\")\n",
    "commentsDfm <- dfm(toks, remove = quanteda::stopwords(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the frequency of terms in the documents\n",
    "TermFreq <- textstat_frequency(commentsDfm)\n",
    "TermFreq[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by total occurances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we want to use the DFM we created to visualize the most freqeuent tokens across all comments. First, we order\n",
    "the tokens, then we plot their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by reverse frequency order\n",
    "TermFreq$feature <- with(TermFreq, reorder(feature, -frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the x most common tokens (you can change x to suit your needs)\n",
    "x <- 15\n",
    "\n",
    "ggplot(TermFreq[1:x], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview might be biased because it just counts the total sum of accurances of each tokens across all comments. It might thus be that there is only one person spamming the same word hundreds of times in a single comment. To mitigate this, we will also count the number of comments that contain each token at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering by occurance in dcuments instead of total frequency\n",
    "DocFreq <- TermFreq[order(-TermFreq$docfreq),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the x tokens that are used in the highest number of comments (you can change x to suit your needs)\n",
    "x <- 15\n",
    "\n",
    "ggplot(DocFreq[1:x], aes(x = feature, y = docfreq)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the most frequent terms, we might want to exclude certain terms that are not indicative for the comments (e.g. the word \"video\") or certain words that are used by spammers (e.g. \"viagra\"). Which words to exclude is the individual decision of each researcher. You should be carefull with designing this list for your needs and be transparent which words you excluded and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Stopword List\n",
    "CustomStops <- c(\"d\",\"xd\",\"oh\",\"lol\")\n",
    "\n",
    "# This is just an example, you should carefully create your own list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create another document-frequency matrix that excludes the Custom Stopwords that we just defined, and then rerun the code above to update our results\n",
    "commentsDfm <- dfm(toks, remove = c(quanteda::stopwords(\"english\"),CustomStops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerunning steps from above in one cell with new DFM (excluding custom stop words)\n",
    "TermFreq <- textstat_frequency(commentsDfm)\n",
    "\n",
    "TermFreq$feature <- with(TermFreq, reorder(feature, -frequency))\n",
    "\n",
    "x <- 25\n",
    "\n",
    "ggplot(TermFreq[1:x], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the DFM to very easily create a wordcloud to visualize the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for most frequently used Terms\n",
    "set.seed(12345)\n",
    "textplot_wordcloud(dfm_select(commentsDfm, min_nchar=3),\n",
    "                   random_order=FALSE,\n",
    "                   max_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Sentiment Analysis (https://en.wikipedia.org/wiki/Sentiment_analysis) on the comments to get an intuition about peoples opinions towards the content of the videos. Sentiment analysis works by comparing the tokens in each comment to a dictionary of words with an attached sentiment rating. For example, the word \"fuck\" would have a negative sentiment rating in the dictionary while the word \"love\" would have a positive sentiment rating. If we add the sentiment scores of all tokens in a given comment, we get an overall sentiment of that comment.\n",
    "\n",
    "Of course, the results dependend on the kind of dictionary that is used. We chose the AFINN dictionary (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), because it is\n",
    "based on the language in microblogs and thus might capture the slang/tone of online comments better than other dictionaries.\n",
    "\n",
    "However, the choice of the dictionary is up to the individual researcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any punctuation and numbers from the vector from obscure Emojis that are not contained in the Emoji database,\n",
    "# for example https://emojipedia.org/reversed-thumbs-up-sign/\n",
    "FormattedComments$TextEmojiDeleted <- gsub(\"[[:punct:][:blank:]]+\", \" \", FormattedComments$TextEmojiDeleted)\n",
    "FormattedComments$TextEmojiDeleted <- gsub('[[:digit:]]+', '', FormattedComments$TextEmojiDeleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.list(head(FormattedComments$TextEmojiDeleted,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting sentiment scores per comment\n",
    "CommentSentiment <- syuzhet::get_sentiment(FormattedComments$TextEmojiDeleted, method = \"afinn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get some summary statistics and a basic vizualisation of sentiment across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "summary(CommentSentiment)\n",
    "boxplot(CommentSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually inspect comments that have extreme ratings as it´s always good to check outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying comments with a sentiment score below x\n",
    "x  <- -10\n",
    "as.list(FormattedComments$TextEmojiDeleted[CommentSentiment < x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disyplaying comments with a sentiment score above x\n",
    "x <- 10\n",
    "as.list(FormattedComments$TextEmojiDeleted[CommentSentiment > x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying most negative/positive comment\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment == min(CommentSentiment)]\n",
    "FormattedComments$TextEmojiDeleted[CommentSentiment == max(CommentSentiment)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Comment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better overview, we can also display the total amount of positive, negative and neutral comments. To this end,\n",
    "we need to create a new dataframe as a crutch to categorize the comments first.\n",
    "\n",
    "We can also display the total distribution of sentiment in comments and overlay the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building helper Frame\n",
    "Desc <- CommentSentiment\n",
    "Desc[Desc > 0] <- \"positive\"\n",
    "Desc[Desc < 0] <- \"negative\"\n",
    "Desc[Desc == 0] <- \"neutral\"\n",
    "df <- data.frame(FormattedComments$TextEmojiDeleted,CommentSentiment,Desc)\n",
    "colnames(df) <- c(\"Comment\",\"Sentiment\",\"Desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the amount of positive, negative and neutral comments\n",
    "ggplot(data=df, aes(x=Desc, fill = Desc)) +\n",
    "        geom_bar(stat='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of comment sentiments\n",
    "ggplot(df, aes(x=Sentiment)) +\n",
    "        geom_histogram(binwidth = 1) +\n",
    "        geom_vline(aes(xintercept=mean(Sentiment)),\n",
    "           color=\"black\", linetype=\"dashed\", size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we only analyzed the text of the comments but we can also analyze the used Emojis in the comments.\n",
    "To this end, we first format NA values correctly for comments that do not contain any Emojis.\n",
    "Second, we tokenize the Emojis just as we did with the text strings. Then, we build an EmojiFreqeuncy Matrix\n",
    "that counts how often each Emojis is contained in every comment. Lastly, we visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting NA´s correctly\n",
    "FormattedComments$Emoji[FormattedComments$Emoji == \"NA\"] <- NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing spaces at the end of the string\n",
    "FormattedComments$Emoji <- substr(FormattedComments$Emoji, 1, nchar(FormattedComments$Emoji)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokinizing\n",
    "EmojiToks <- tokens(FormattedComments$Emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the Emojis in the first 10 comments\n",
    "EmojiToks[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build an Emoji Frequency Matrix, excluding \"NA\" as a term\n",
    "EmojiDfm <- dfm(EmojiToks,remove = \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the frequency of Emojis in the documents\n",
    "EmojiFreq <- textstat_frequency(EmojiDfm)\n",
    "EmojiFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get a more sparse overview of the x top Emojis in the comments\n",
    "x = 20\n",
    "topfeatures(EmojiDfm,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by total occurances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the EmojiFrequency Matrix, we can plot the most frequently occuring Emojis across all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by reverse frequency order\n",
    "EmojiFreq$feature <- with(EmojiFreq, reorder(feature, -frequency))\n",
    "\n",
    "# Defining x\n",
    "x <- 30\n",
    "\n",
    "# Plotting x most common Emojis\n",
    "ggplot(EmojiFreq[1:x], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a nice way of visulaizing the distribution, we can use the emoGG package to replace the points in the scatterplot\n",
    "with the actual Emojis that they represent. For this, we have to create a mapping for each. Because this involves searching the\n",
    "database manually for the emoji hex codes used by the emoGG package, we will restrict ourselves here to the 10 most commonly used Emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping1 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_facewithtearsofjoy\",], aes(feature,frequency), emoji = \"1f602\")\n",
    "mapping2 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_hamburger\",], aes(feature,frequency), emoji = \"1f354\")\n",
    "mapping3 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_frenchfries\",], aes(feature,frequency), emoji = \"1f35f\")\n",
    "mapping4 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_smilingfacewithsunglasses\",], aes(feature,frequency), emoji = \"1f60e\")\n",
    "mapping5 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_smilingface\",], aes(feature,frequency), emoji = \"263a\")\n",
    "mapping6 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_fire\",], aes(feature,frequency), emoji = \"1f525\")\n",
    "mapping7 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_loudlycryingface\",], aes(feature,frequency), emoji = \"1f62d\")\n",
    "mapping8 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_smilingfacewithheart-eyes\",], aes(feature,frequency), emoji = \"1f60d\")\n",
    "mapping9 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_rollingonthefloorlaughing\",], aes(feature,frequency), emoji = \"1f923\")\n",
    "mapping10 <- geom_emoji(data = EmojiFreq[EmojiFreq$feature == \"emoji_redheart\",], aes(feature,frequency), emoji = \"2764\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by reverse frequency order\n",
    "EmojiFreq$feature <- with(EmojiFreq, reorder(feature, -frequency))\n",
    "\n",
    "# Plotting x most common Emojis\n",
    "ggplot(EmojiFreq[1:10], aes(x = feature, y = frequency)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n",
    "        mapping1 +\n",
    "        mapping2 +\n",
    "        mapping3 +\n",
    "        mapping4 +\n",
    "        mapping5 +\n",
    "        mapping6 +\n",
    "        mapping7 +\n",
    "        mapping8 +\n",
    "        mapping9 +\n",
    "        mapping10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing by number of comments containing the Emoji at least once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overview might be biased because it just counts the total sum of accurances of each Emoji across all comments. It might thus be that there is only one person spamming the same Emoji hundreds of times in a single comment. To mitigate this, we will also count the number of comments that contain each Emoji at least once.\n",
    "\n",
    "Basically, we´re counting the number of comments that do contain the Emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by reverse document frequency order\n",
    "EmojiFreq$feature <- with(EmojiFreq, reorder(feature, -docfreq))\n",
    "\n",
    "# Plotting 30 most frequent Emoji\n",
    "ggplot(EmojiFreq[1:30], aes(x = feature, y = docfreq)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new frame order by document occurance frequenc rather than overall frequency\n",
    "NewOrder <- EmojiFreq[order(-EmojiFreq$docfreq),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can visualize this nicely by using the repective emojis instead of the normal scatterplot points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping1 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_facewithtearsofjoy\",], aes(feature,docfreq), emoji = \"1f602\")\n",
    "mapping2 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_hamburger\",], aes(feature,docfreq), emoji = \"1f354\")\n",
    "mapping3 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_loudlycryingface\",], aes(feature,docfreq), emoji = \"1f62d\")\n",
    "mapping4 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_fire\",], aes(feature,docfreq), emoji = \"1f525\")\n",
    "mapping5 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_redheart\",], aes(feature,docfreq), emoji = \"2764\")\n",
    "mapping6 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_heartsuit\",], aes(feature,docfreq), emoji = \"2665\")\n",
    "mapping7 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_frenchfries\",], aes(feature,docfreq), emoji = \"1f35f\")\n",
    "mapping8 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_rollingonthefloorlaughing\",], aes(feature,docfreq), emoji = \"1f923\")\n",
    "mapping9 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_thumbsup\",], aes(feature,docfreq), emoji = \"1f44d\")\n",
    "mapping10 <- geom_emoji(data = NewOrder[NewOrder$feature == \"emoji_smilingfacewithheart-eyes\",], aes(feature,docfreq), emoji = \"1f60d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting 30 most frequent Emoji\n",
    "ggplot(NewOrder[1:10], aes(x = feature, y = docfreq)) +\n",
    "        geom_point() + \n",
    "        theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n",
    "        mapping1 +\n",
    "        mapping2 +\n",
    "        mapping3 +\n",
    "        mapping4 +\n",
    "        mapping5 +\n",
    "        mapping6 +\n",
    "        mapping7 +\n",
    "        mapping8 +\n",
    "        mapping9 +\n",
    "        mapping10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Emojis (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like text (and arguably even more so), Emojis are used to confer emotions and opinion. For this reason, we´re trying here for an explorative sentiment analysis using the Emojis in the comments. Just as for the sentences,we thus need a dicitionary that maps Emojis to Sentiments. Unfortunately, there so far seems to be only one sentiment dictionary for  the most commonly used 734 Emojis (http://kt.ijs.si/data/Emoji_sentiment_ranking/) . Thus, we currently do not have the possibility to check the results with different dictionaries and cannot inlcude all Emojis in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing emojis dictionary (We only get 734 different Emojis but thats the best data we have on Emoji Sentiment)\n",
    "EmojiSentiments <- lexicon::emojis_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the first ten rows of the Emoji Sentiment dictionary\n",
    "EmojiSentiments[1:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to match the sentiment scores to our codings of the emojis and create a quanteda dictionary object\n",
    "EmojiNames <- paste0(\"emoji_\",gsub(\" \",\"\",EmojiSentiments$name))\n",
    "EmojiSentiment <- cbind.data.frame(EmojiNames,EmojiSentiments$sentiment,EmojiSentiments$polarity)\n",
    "names(EmojiSentiment) <- c(\"word\",\"sentiment\",\"valence\")\n",
    "EmojiSentDict <- as.dictionary(EmojiSentiment[,1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the Emoji-only column in our formatted dataframe\n",
    "EmojiToks <- tokens(tolower(FormattedComments$Emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now replace the emojis in the dictionary with the corresponding sentiment scores\n",
    "EmojiToksSent <- tokens_lookup(x = EmojiToks, dictionary = EmojiSentDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many Emoji we can cover with sentiment scores\n",
    "UnlistedEmojiToksSent <- unlist(EmojiToksSent)\n",
    "names(UnlistedEmojiToksSent) <- NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After mapping the Emojis in our dataset to the sentiment scores in the dictionary, we can check how many Emojis we have in total in our dataset and how many of those we are getting sentiment mappings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total Emoji\n",
    "NumberOfEmojiSentiments <- UnlistedEmojiToksSent[UnlistedEmojiToksSent!=\"NA\"]\n",
    "length(NumberOfEmojiSentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sanity check\n",
    "\n",
    "# Number of Emoji that couldn´t be replaced\n",
    "length(grep(\"emoji_\",NumberOfEmojiSentiments))\n",
    "\n",
    "# number of Emoji that could be replaced\n",
    "length(grep(\"0.\",NumberOfEmojiSentiments))\n",
    "\n",
    "# Percentage of Emoji that couldn´t be replaced\n",
    "length(grep(\"emoji_\",NumberOfEmojiSentiments))/length(NumberOfEmojiSentiments)\n",
    "\n",
    "# Percentage of Emoji that could be replaced\n",
    "length(grep(\"0.\",NumberOfEmojiSentiments))/length(NumberOfEmojiSentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to add sentiments for Emojis within the same comment to get an overall comment sentiment based on Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing sentiment scores for comments based on Emoji\n",
    "\n",
    "# only keeping the replaced sentiment scores for th Emoji vector\n",
    "ReplacedEmojiSentScores <- tokens_select(EmojiToksSent,EmojiSentiment$sentiment,\"keep\")\n",
    "ReplacedEmojiSentScores <- as.list(ReplacedEmojiSentScores)\n",
    "\n",
    "# function to add sentiment scores of Emojis per comment\n",
    "AddEmojiSentiments <- function(x){\n",
    "        \n",
    "        x <- sum(as.numeric(as.character(x)))\n",
    "        return(x)\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "AdditiveEmojiSentiment <- lapply(ReplacedEmojiSentScores,AddEmojiSentiments)\n",
    "AdditiveEmojiSentiment[AdditiveEmojiSentiment == 0] <- NA\n",
    "AdditiveEmojiSentiment <- unlist(AdditiveEmojiSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting histogram for distribution of Emoji Sentiment Scores\n",
    "hist(AdditiveEmojiSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation between Emoji sentiment score and text Sentiment Score\n",
    "cor(CommentSentiment,AdditiveEmojiSentiment,use=\"complete.obs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting the relationship\n",
    "plot(CommentSentiment,AdditiveEmojiSentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
